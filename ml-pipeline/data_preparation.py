import pandas as pd
import numpy as np
import cv2
import os
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import json

class CattleDataProcessor:
    def __init__(self, dataset_path: str):
        self.dataset_path = Path(dataset_path)
        self.images_path = self.dataset_path / "images"
        self.annotations_path = self.dataset_path / "annotations"
        
    def analyze_dataset(self):
        """Dataset'i analiz et ve istatistikleri Ã§Ä±kar"""
        print("ğŸ” Dataset analizi baÅŸlatÄ±lÄ±yor...")
        
        # GÃ¶rsel dosyalarÄ±nÄ± say
        image_files = list(self.images_path.glob("*.jpg")) + list(self.images_path.glob("*.png"))
        annotation_files = list(self.annotations_path.glob("*.json")) + list(self.annotations_path.glob("*.csv"))
        
        print(f"ğŸ“Š Toplam gÃ¶rsel sayÄ±sÄ±: {len(image_files)}")
        print(f"ğŸ“‹ Toplam annotasyon sayÄ±sÄ±: {len(annotation_files)}")
        
        # GÃ¶rsel boyutlarÄ±nÄ± analiz et
        image_sizes = []
        weights = []
        
        for i, img_path in enumerate(image_files[:100]):  # Ä°lk 100 gÃ¶rsel iÃ§in
            img = cv2.imread(str(img_path))
            if img is not None:
                image_sizes.append(img.shape[:2])
                
                # Corresponding annotation file bul
                annotation_file = self.annotations_path / f"{img_path.stem}.json"
                if annotation_file.exists():
                    with open(annotation_file, 'r') as f:
                        data = json.load(f)
                        if 'weight' in data:
                            weights.append(data['weight'])
        
        # Ä°statistikleri gÃ¶ster
        if weights:
            print(f"ğŸ“ AÄŸÄ±rlÄ±k ortalamasÄ±: {np.mean(weights):.2f} kg")
            print(f"ğŸ“ AÄŸÄ±rlÄ±k aralÄ±ÄŸÄ±: {np.min(weights):.2f} - {np.max(weights):.2f} kg")
            
        if image_sizes:
            heights, widths = zip(*image_sizes)
            print(f"ğŸ–¼ï¸ Ortalama gÃ¶rsel boyutu: {np.mean(widths):.0f}x{np.mean(heights):.0f}")
            
        return {
            'total_images': len(image_files),
            'total_annotations': len(annotation_files),
            'weight_stats': {
                'mean': np.mean(weights) if weights else 0,
                'min': np.min(weights) if weights else 0,
                'max': np.max(weights) if weights else 0,
                'std': np.std(weights) if weights else 0
            },
            'image_stats': {
                'mean_width': np.mean(widths) if image_sizes else 0,
                'mean_height': np.mean(heights) if image_sizes else 0
            }
        }
    
    def prepare_training_data(self, target_size=(224, 224)):
        """EÄŸitim verisi hazÄ±rla"""
        print("ğŸ”„ EÄŸitim verisi hazÄ±rlanÄ±yor...")
        
        X = []  # GÃ¶rseller
        y = []  # AÄŸÄ±rlÄ±klar
        metadata = []  # Ek bilgiler
        
        image_files = list(self.images_path.glob("*.jpg")) + list(self.images_path.glob("*.png"))
        
        for img_path in image_files:
            # GÃ¶rsel yÃ¼kle ve resize et
            img = cv2.imread(str(img_path))
            if img is not None:
                img_resized = cv2.resize(img, target_size)
                img_normalized = img_resized / 255.0
                
                # Annotation dosyasÄ±nÄ± bul
                annotation_file = self.annotations_path / f"{img_path.stem}.json"
                if annotation_file.exists():
                    with open(annotation_file, 'r') as f:
                        data = json.load(f)
                        
                        if 'weight' in data:
                            X.append(img_normalized)
                            y.append(data['weight'])
                            metadata.append({
                                'filename': img_path.name,
                                'animal_type': data.get('animal_type', 'unknown'),
                                'age': data.get('age', 0),
                                'gender': data.get('gender', 'unknown')
                            })
        
        # Numpy array'e Ã§evir
        X = np.array(X)
        y = np.array(y)
        
        print(f"âœ… HazÄ±rlanan veri: {X.shape[0]} Ã¶rnek")
        print(f"ğŸ“Š GÃ¶rsel boyutu: {X.shape[1:]}") 
        print(f"ğŸ¯ Hedef deÄŸiÅŸken aralÄ±ÄŸÄ±: {y.min():.2f} - {y.max():.2f} kg")
        
        return X, y, metadata
    
    def create_data_splits(self, X, y, metadata, test_size=0.2, val_size=0.1):
        """Veriyi train/validation/test olarak bÃ¶l"""
        print("ğŸ“‹ Veri bÃ¶lÃ¼nÃ¼yor...")
        
        # Ä°lk olarak train+val ve test ayÄ±r
        X_temp, X_test, y_temp, y_test, meta_temp, meta_test = train_test_split(
            X, y, metadata, test_size=test_size, random_state=42
        )
        
        # Train ve validation ayÄ±r
        val_size_adjusted = val_size / (1 - test_size)
        X_train, X_val, y_train, y_val, meta_train, meta_val = train_test_split(
            X_temp, y_temp, meta_temp, test_size=val_size_adjusted, random_state=42
        )
        
        print(f"ğŸ”¹ Train set: {X_train.shape[0]} Ã¶rnek")
        print(f"ğŸ”¹ Validation set: {X_val.shape[0]} Ã¶rnek") 
        print(f"ğŸ”¹ Test set: {X_test.shape[0]} Ã¶rnek")
        
        return {
            'train': (X_train, y_train, meta_train),
            'val': (X_val, y_val, meta_val),
            'test': (X_test, y_test, meta_test)
        }
    
    def save_processed_data(self, data_splits, output_dir="processed_data"):
        """Ä°ÅŸlenmiÅŸ veriyi kaydet"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        for split_name, (X, y, metadata) in data_splits.items():
            np.save(output_path / f"X_{split_name}.npy", X)
            np.save(output_path / f"y_{split_name}.npy", y)
            
            with open(output_path / f"metadata_{split_name}.json", 'w') as f:
                json.dump(metadata, f, indent=2)
        
        print(f"ğŸ’¾ Ä°ÅŸlenmiÅŸ veri kaydedildi: {output_path}")

if __name__ == "__main__":
    # Dataset yolunu belirt
    dataset_path = "cattle_weight_dataset"
    
    processor = CattleDataProcessor(dataset_path)
    
    # 1. Dataset analizi
    stats = processor.analyze_dataset()
    print(f"ğŸ“ˆ Dataset istatistikleri: {stats}")
    
    # 2. EÄŸitim verisi hazÄ±rla
    X, y, metadata = processor.prepare_training_data()
    
    # 3. Veriyi bÃ¶l
    data_splits = processor.create_data_splits(X, y, metadata)
    
    # 4. Kaydet
    processor.save_processed_data(data_splits)
    
    print("âœ… Veri hazÄ±rlÄ±ÄŸÄ± tamamlandÄ±!") 